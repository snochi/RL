\documentclass[RL]{subfiles}

%% ========================================================
%% document

\begin{document}

    \section{Markov Decision Processes}

    As mentioned in Section 1, \textit{Markov decision processes} (MDPs) formally describe an environment for RL when the environment is \textit{fully observable}. That is, the current state completely characterizes the process. The nice thing about MDPs is that most RL problems can be formalized in to MDPs. For instance,
    \begin{enumerate}
        \item optimal constrol primarily deals with continuous MDPs;
        \item partially observable problems can be converted in to MDPs; and
        \item bandits are MDPs with one state. 
    \end{enumerate}

    \subsection{Markov Processes}

    Let us start by recalling the definition of \textit{Markov property}.

    \begin{recall}{\textbf{Markov Process}}
        A \emph{stochastic process} is a sequence $\left( X_{t} \right)^{}_{t}$ of random variables.

        We say a stochastic process $\left( S_{t} \right)^{}_{t}$ is \emph{Markov} if
        \begin{equation*}
            \PP\left( S_{t+1}|S_t,\ldots,S_1 \right) = \PP\left( S_{t+1}|S_t \right).
        \end{equation*}
    \end{recall}

    \np For a Markov state $s$ and successor state $s'$, the \emph{state transition probability}, denoted as $\mP_{ss'}$, is defined by
    \begin{equation*}
        \mP_{ss'} = \PP\left( S_{t+1}=s' | S_t=s \right).
    \end{equation*}
    The \emph{state transition matrix} $\mP$ defines transition probabilities from all states $s$ to all successor states $s'$,
    \begin{equation*}
        \mP = 
        \begin{bmatrix}
            \mP_{11} & \cdots & \mP_{1n} \\
            \vdots & \ddots & \vdots \\
            \mP_{n1} & \cdots & \mP_{nn} \\
        \end{bmatrix}.
    \end{equation*}
    This means we can characterize a Markov process $\left( S_{t} \right)^{}_{t}$ by a pair $\left( \mS,\mP \right)$, where $\mS$ is the (finite) set ofpossible states and $\mP$ is the state transition matrix. For this reason, we shall also call $\left( \mS,\mP \right)$ a \emph{Markov process}.

    \begin{definition}{\textbf{Markov Reward Process}}
        A \emph{Markov reward process} (\emph{MRP}) is a tuple $\left( \mS,\mP,\mR,\gamma \right)$ such that 
        \begin{enumerate}
            \item $\left( \mS,\mP \right)$ is a Markov process;
            \item $\mR$ is a \emph{reward function} with
                \begin{equation*}
                    \mR_s = \EE\left( R_{t+1}|S_t=s \right)
                \end{equation*}
                for all $s\in\mS$;\footnotemark[1] and
            \item $\gamma\in\left[ 0,1 \right]$, called the \emph{discount factor}.
        \end{enumerate}
        
        \noindent
        \begin{minipage}{\textwidth}
            \footnotetext[1]{$R_{t}$ represents the \emph{reward} at step $t$.}
        \end{minipage}
    \end{definition}

    \begin{definition}{\textbf{Return}}
        The \emph{return} from step $t$, denoted as $G_t$, is the total discounted reward from $t$:
        \begin{equation*}
            G_t = \sum^{\infty}_{k=0}\gamma^k R_{t+k+1} = R_{t+1} + \gamma R_{t+2} + \gamma^{2}R_{t+3} + \cdots.
        \end{equation*}
    \end{definition}

    \np According to the definition of $G_t$, the value of receiving reward $R$ after $k+1$ steps is $\gamma^kR$, so $G_t$ values immediate reward above delayed reward. Specifically, $\gamma\approx 0$ leads to \emph{myopic} evaluation whereas $\gamma\approx 1$ leads to \emph{far-sighted} evaluation.

    \np One reason that we use a discount factor is because of the \textit{uncertainty} in the future. To put this in another way, we \textit{do not} have a perfect model of the environment. 

    Another reason is for mathematical convenience: by using discount rewards, we can avoid infinte returns (e.g. in cyclic Markov processes).

    \begin{definition}{\textbf{State Value Function}}
        The \emph{state value function}, denoted as $v\left( s \right)$, of an MRP is the expectation of $G_t$ starting from state $s$:
        \begin{equation*}
            v\left( s \right) = \EE\left( G_t|S_t=s \right)
        \end{equation*}
        for all state $s$.
    \end{definition}

    \np By defintion, we have
    \begin{flalign*}
        && v\left( s \right) & = \EE\left( G_t | S_t=s \right) && \\ 
        && & = \EE\left( R_{t+1} + \gamma R_{t+2} + \gamma^{2} R_{t+3} + \cdots | S_t=s \right) && \\
        && & = \EE\left( R_{t+1} + \gamma\left( R_{t+2} + \gamma R_{t+3}+\cdots \right) | S_t=s \right) && \\
        && & = \EE\left( R_{t+1} + \gamma G_{t+1} | S_t=s \right) && \\
        && & = \EE\left( R_{t+1} + \gamma v\left( S_{t+1} \right)|S_t=s \right).
    \end{flalign*}
    That is, the value function $v\left( s \right)$ has two parts: immediate reward $R_{t+1}$ and discount value of successor state $\gamma v\left( S_{t+1} \right)$. The equation
    \begin{equation}
        v\left( s \right) = \EE\left( R_{t+1}+\gamma v\left( S_{t+1} \right)|S_t=s \right)
    \end{equation}
    is called the \textit{Bellman equation}. This also can be expressed concisely using matrices:
    \begin{equation}
        v = \mR+\gamma\mP v,
    \end{equation}
    where $v$ is a column vector with on entry per state. That is,
    \begin{equation}
        \begin{bmatrix} v\left( 1 \right)\\\vdots\\v\left( n \right) \end{bmatrix} = \begin{bmatrix} \mR_1\\\vdots\\\mR_n \end{bmatrix} + \gamma
        \begin{bmatrix}
            \mP_{11} & \cdots & \mP_{1n} \\
        	\vdots & \ddots & \vdots \\
                \mP_{n1} & \cdots & \mP_{nn} \\
        \end{bmatrix}
        \begin{bmatrix} v\left( 1 \right)\\\vdots\\v\left( n \right) \end{bmatrix}.
    \end{equation}
    [2.3] is a linear equation, which can be solved directly:
    \begin{equation}
        v = \left( I_n-\gamma\mP \right)^{-1}\mR.
    \end{equation}
    Evaluating [2.4] has time complexity $O\left( n^{3} \right)$, so direct solution is only feasible for small MRPs. Instead, we will shortly look into many iterative (approximation) methods for larger MRPs, such as
    \begin{enumerate}
        \item dynamic programming;
        \item Monte-Carlo evaluation; and
        \item temporal-difference learning.
    \end{enumerate}
    
    \clearpage
    \subsection{Markov Decision Processes}
    
    \begin{definition}{\textbf{Markov Decision Process}}
        A \emph{Markov decision process} (\emph{MDP}) is a tuple $\left( \mS,\mA,\mP,\mR,\gamma \right)$ such that
        \begin{enumerate}
            \item $\left( \mS,\mP,\mR,\gamma \right)$ is a MRP with additional properties
                \begin{equation*}
                    \mP_{ss'}^a = \PP\left( S_{t+1}=s'|S_t=s, A_t=a \right)
                \end{equation*}
                and
                \begin{equation*}
                    \mR^a_{s} = \EE\left( R_{t+1}|S_t=s, A_t=a \right)
                \end{equation*}
                for all states $s,s'$ and action $a$; and
            \item $\mA$ is a (finite) set of actions.
        \end{enumerate}
    \end{definition}

    In short, a MDP is a MRP with decisions.

    \begin{definition}{\textbf{Policy}}
        A (stochastic) \emph{policy} $\pi$ is a distribution over actions given states,
        \begin{equation*}
            \pi\left( a|s \right) = \PP\left( A_t=a|S_t=s \right)
        \end{equation*}
        for all $a\in\mA,s\in\mS$.
    \end{definition}

    \np A policy fully characterizes the behavior of an agent. In an MDP, policies depend on the current state but not the whole history. In particular, policies are \textit{stationary} (i.e. time-independent). That is, given any $t,t'>0$,
    \begin{equation*}
        \PP\left( A_{t'}=a|S_{t'}=s \right) = \PP\left( A_t=a|S_t=s \right).
    \end{equation*}

    \np Suppose that an MDP $\mM = \left( \mS,\mA,\mP,\mR,\gamma \right)$ and a policy $\pi$ are given. Then the state sequence $\left( S_{t} \right)^{}_{t}$ is a Markov process $\left( \mS,\mP^{\pi} \right)$ and the state and reward sequence $\left( \left( S_t,R_t \right) \right)_t$ is a MRP $\left( \mS,\mP^{\pi},\mR^{\pi},\gamma \right)$, where
    \begin{equation*}
        \mP^{\pi}_{s,s'} = \sum^{}_{a\in\mA} \pi\left( a|s \right)\mP^a_{s,s'}
    \end{equation*}
    and
    \begin{equation*}
        \mR^{\pi}_s = \sum^{}_{a\in\mA} \pi\left( a|s \right)\mR^a_s.
    \end{equation*}
    This leads to the following two definitions.

    \begin{definition}{\textbf{State-value Function}, \textbf{Action-value Function} of an MDP}
        The \emph{state-value function} $v_{\pi}\left( s \right)$ of an MDP is the expectation of return starting from state $s$ and then following policy $\pi$:
        \begin{equation*}
            v_{\pi}\left( s \right) = \EE_{\pi}\left( G_t|S_t=s \right).
        \end{equation*}

        The \emph{action-value function} $q_{\pi}\left( s,a \right)$ is the expected return starting from state $s$, taking action $a$, and ten following policy $\pi$:
        \begin{equation*}
            q_{\pi}\left( s,a \right) = \EE_{\pi}\left( G_t|S_t=s, A_t=a \right).
        \end{equation*}
    \end{definition}

    \np Similar to [2.1], we can decompose the state-value function $v_{\pi}\left( s \right)$ and the action-value function $q_{\pi}\left( s,a \right)$ into two parts:
    \begin{equation}
        v_{\pi}\left( s \right) = \EE_{\pi}\left( R_{t+1}+\gamma v_{\pi}\left( S_{t+1} \right) | S_t=s \right)
    \end{equation}
    and
    \begin{equation}
        q_{\pi}\left( s,a \right) = \EE_{\pi}\left( R_{t+1}+\gamma q_{\pi}\left( S_{t+1},A_{t+1} \right)|S_t=s,A_t=a \right).
    \end{equation}
    Writing [2.5] in matrix notation gives
    \begin{equation}
        v_{\pi} = \mR^{\pi} + \gamma\mP^{\pi}v_{\pi}
    \end{equation}
    with an explicit solution
    \begin{equation}
        v_{\pi} = \left( I_n-\gamma\mP^{\pi} \right)^{-1}\mR^{\pi}.
    \end{equation}

    \begin{definition}{\textbf{Optimal State-value Function}, \textbf{Optimal Action-value Function} of an MDP}
        The \emph{optimal state-value function} $v_{*}\left( s \right)$ is the maximum state-value function over all policies:
        \begin{equation*}
            v_{*}\left( s \right) = \max_{\pi} v_{\pi}\left( s \right).
        \end{equation*}

        The \emph{optimal action-value function} $q_{*}\left( s,a \right)$ is the maximum action-value function over all policies:
        \begin{equation*}
            q_{*}\left( s,a \right) = \max_{\pi} q_{\pi}\left( s,a \right).
        \end{equation*}
    \end{definition}

    \np We can define the following partial order on policies that captures the idea of \textit{a policy better than another}: we write
    \begin{equation*}
        \pi\geq\pi' 
    \end{equation*}
    if for every $s\in\mS$, $v_{\pi}\left( s \right)\geq v_{\pi'}\left( s \right)$. The following theorem concerns this partial order.

    \begin{theorem}{}
        For any MDP,
        \begin{enumerate}
            \item there is an optimal policy $\pi_{*}$ such that $\pi_{*}\geq \pi$ for all policy $\pi$; and
            \item any optimal policy $\pi_*$ achieve the optimal state-vaue function, 
                \begin{equation*}
                    v_{\pi_*} = v_*,
                \end{equation*}
                and the optimal action-value function,
                \begin{equation*}
                    q_{\pi_*} = q_*.
                \end{equation*}
        \end{enumerate}
    \end{theorem}

    \rruleline

    \np The optimal value functions are recursively related by the Bellman optimality equations:
    \begin{equation}
        v_*\left( s \right) = \max_{a} q_*\left( s,a \right)
    \end{equation}
    and
    \begin{equation}
        q_*\left( s,a \right) = \mR^a_s + \gamma\sum^{}_{s'\in\mS}\mP^a_{ss'}v_*\left( s' \right).
    \end{equation}
    This means
    \begin{equation}
        v_*\left( s \right) = \max_a \mR^a_s + \gamma\sum^{}_{s'\in\mS}\mP^a_{ss'}v_*\left( s' \right)
    \end{equation}
    and
    \begin{equation}
        q_*\left( s,a \right) = \mR^a_s + \gamma\sum^{}_{s'\in\mS} \mP^a_{ss'} \max_{a'}q_*\left( s',a' \right).
    \end{equation}
    [2.9] and [2.10] are non-linear, and they do not have no closed form solution in general. Instead, we use iterative approximation methods.





























    
    
    
    
    
    
    
    
    
    

\end{document}
